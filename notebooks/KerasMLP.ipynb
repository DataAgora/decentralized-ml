{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Multi-layer Perceptron\n",
    "\n",
    "Nothing crazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports (from the UNIX Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class GenericModel:\n",
    "    \"\"\"\n",
    "    Generic Tensorflow Model Class\n",
    "\n",
    "    Each subclass of this class needs to define the data structure of its weights\n",
    "    (which should be respected accross methods) and implement the functions below.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def get_model(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def load_weights(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def get_weights(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def sum_weights(self, weights1, weights2):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def scale_weights(self, weights, factor):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "    def inverse_scale_weights(self, weights, factor):\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "\n",
    "class GenericKerasModel(GenericModel):\n",
    "    def set_weights(self, new_weights):\n",
    "        self.model.set_weights(new_weights)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def get_initial_weights(self):\n",
    "        model = self.build_model()\n",
    "        return model.get_weights()\n",
    "\n",
    "    def sum_weights(self, weights1, weights2):\n",
    "        new_weights = []\n",
    "        for w1, w2 in zip(weights1, weights2):\n",
    "            new_weights.append(w1 + w2)\n",
    "        return new_weights\n",
    "\n",
    "    def scale_weights(self, weights, factor):\n",
    "        new_weights = []\n",
    "        for w in weights:\n",
    "            new_weights.append(w * factor)\n",
    "        return new_weights\n",
    "\n",
    "    def inverse_scale_weights(self, weights, factor):\n",
    "        new_weights = []\n",
    "        for w in weights:\n",
    "            new_weights.append(w / factor)\n",
    "        return new_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "class KerasPerceptron(GenericKerasModel):\n",
    "    def __init__(self, is_training=False):\n",
    "        self.n_input = 784\n",
    "        self.n_hidden1 = 200\n",
    "        self.n_hidden2 = 200\n",
    "        self.n_classes = 10\n",
    "        self.is_training = is_training\n",
    "        self.model = self.build_model()\n",
    "        if is_training:\n",
    "            self.compile_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.n_hidden1, input_shape=(self.n_input,), activation='relu', kernel_initializer=keras.initializers.glorot_uniform()))\n",
    "        model.add(Dense(self.n_hidden2, activation='relu', kernel_initializer=keras.initializers.glorot_uniform()))\n",
    "        model.add(Dense(self.n_classes, activation='softmax', kernel_initializer=keras.initializers.glorot_uniform()))\n",
    "        # model.summary()\n",
    "        return model\n",
    "\n",
    "    def compile_model(self):\n",
    "        sgd = optimizers.SGD(lr=0.001)\n",
    "        self.model.compile(\n",
    "            optimizer=sgd,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom saving/loading for Keras.\n",
    "Based on https://github.com/keras-team/keras/blob/master/keras/engine/saving.py.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "def model_from_serialized(serialized_model):\n",
    "    uncompiled_model = model_from_json(serialized_model['architecture'])\n",
    "    return _load_optimizer(uncompiled_model, serialized_model['optimizer'])\n",
    "\n",
    "def get_optimizer(model):\n",
    "    def get_json_type(obj):\n",
    "        \"\"\"Serialize any object to a JSON-serializable structure.\n",
    "        # Arguments\n",
    "            obj: the object to serialize\n",
    "        # Returns\n",
    "            JSON-serializable structure representing `obj`.\n",
    "        # Raises\n",
    "            TypeError: if `obj` cannot be serialized.\n",
    "        \"\"\"\n",
    "        # if obj is a serializable Keras class instance\n",
    "        # e.g. optimizer, layer\n",
    "        if hasattr(obj, 'get_config'):\n",
    "            return {'class_name': obj.__class__.__name__,\n",
    "                    'config': obj.get_config()}\n",
    "\n",
    "        # if obj is any numpy type\n",
    "        if type(obj).__module__ == np.__name__:\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return {'type': type(obj),\n",
    "                        'value': obj.tolist()}\n",
    "            else:\n",
    "                return obj.item()\n",
    "\n",
    "        # misc functions (e.g. loss function)\n",
    "        if callable(obj):\n",
    "            return obj.__name__\n",
    "\n",
    "        # if obj is a python 'type'\n",
    "        if type(obj).__name__ == type.__name__:\n",
    "            return obj.__name__\n",
    "\n",
    "        raise TypeError('Not JSON Serializable:', obj)\n",
    "\n",
    "    if model.optimizer:\n",
    "        metadata = {}\n",
    "        if isinstance(model.optimizer, optimizers.TFOptimizer):\n",
    "            warnings.warn(\n",
    "                'TensorFlow optimizers do not '\n",
    "                'make it possible to access '\n",
    "                'optimizer attributes or optimizer state '\n",
    "                'after instantiation. '\n",
    "                'As a result, we cannot save the optimizer '\n",
    "                'as part of the model save file.'\n",
    "                'You will have to compile your model again '\n",
    "                'after loading it. '\n",
    "                'Prefer using a Keras optimizer instead '\n",
    "                '(see keras.io/optimizers).')\n",
    "        else:\n",
    "            metadata['training_config'] = json.dumps({\n",
    "                'optimizer_config': {\n",
    "                    'class_name': model.optimizer.__class__.__name__,\n",
    "                    'config': model.optimizer.get_config()\n",
    "                },\n",
    "                'loss': model.loss,\n",
    "                'metrics': model.metrics,\n",
    "                'sample_weight_mode': model.sample_weight_mode,\n",
    "                'loss_weights': model.loss_weights,\n",
    "            }, default=get_json_type)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def _load_optimizer(uncompiled_model, optimizer_metadata):\n",
    "    custom_objects = {}\n",
    "\n",
    "    def convert_custom_objects(obj):\n",
    "        \"\"\"Handles custom object lookup.\n",
    "        # Arguments\n",
    "            obj: object, dict, or list.\n",
    "        # Returns\n",
    "            The same structure, where occurrences\n",
    "                of a custom object name have been replaced\n",
    "                with the custom object.\n",
    "        \"\"\"\n",
    "        if isinstance(obj, list):\n",
    "            deserialized = []\n",
    "            for value in obj:\n",
    "                deserialized.append(convert_custom_objects(value))\n",
    "            return deserialized\n",
    "        if isinstance(obj, dict):\n",
    "            deserialized = {}\n",
    "            for key, value in obj.items():\n",
    "                deserialized[key] = convert_custom_objects(value)\n",
    "            return deserialized\n",
    "        if obj in custom_objects:\n",
    "            return custom_objects[obj]\n",
    "        return obj\n",
    "\n",
    "    # instantiate optimizer\n",
    "    training_config = optimizer_metadata.get('training_config')\n",
    "    if training_config is None:\n",
    "        warnings.warn('No training configuration found in save file: '\n",
    "                      'the model was *not* compiled. '\n",
    "                      'Compile it manually.')\n",
    "        return uncompiled_model\n",
    "    training_config = json.loads(training_config)\n",
    "    optimizer_config = training_config['optimizer_config']\n",
    "    optimizer = optimizers.deserialize(optimizer_config,\n",
    "                                       custom_objects=custom_objects)\n",
    "\n",
    "    # Recover loss functions and metrics.\n",
    "    loss = convert_custom_objects(training_config['loss'])\n",
    "    metrics = convert_custom_objects(training_config['metrics'])\n",
    "    sample_weight_mode = training_config['sample_weight_mode']\n",
    "    loss_weights = training_config['loss_weights']\n",
    "\n",
    "    # Compile model.\n",
    "    uncompiled_model.compile(optimizer=optimizer,\n",
    "                             loss=loss,\n",
    "                             metrics=metrics,\n",
    "                             loss_weights=loss_weights,\n",
    "                             sample_weight_mode=sample_weight_mode)\n",
    "\n",
    "    model = uncompiled_model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 199,210\n",
      "Trainable params: 199,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = KerasPerceptron(is_training=True)\n",
    "m.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train.resize((x_train.shape[0], 28 * 28))\n",
    "x_test.resize((x_test.shape[0], 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Initial Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.02772636, -0.04897893, -0.02742239, ..., -0.04553548,\n",
       "         -0.04263868, -0.00225566],\n",
       "        [-0.04189289, -0.00273237, -0.07241117, ...,  0.03103768,\n",
       "          0.00918383, -0.04232153],\n",
       "        [ 0.00949974, -0.05841019,  0.01852147, ..., -0.0653021 ,\n",
       "         -0.00260213, -0.05574984],\n",
       "        ...,\n",
       "        [-0.03398181,  0.05027232,  0.04795089, ..., -0.01953086,\n",
       "          0.03278097,  0.0472555 ],\n",
       "        [-0.01158489,  0.01730023, -0.03912467, ..., -0.07445896,\n",
       "         -0.02684168, -0.07745838],\n",
       "        [-0.04570943, -0.04665513, -0.07514221, ...,  0.01580391,\n",
       "          0.03125388,  0.01501066]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.07013445,  0.07256729,  0.07262079, ..., -0.04755202,\n",
       "         -0.01928235,  0.11339395],\n",
       "        [-0.01290201,  0.03126728,  0.03236745, ...,  0.01993036,\n",
       "         -0.09450158,  0.02150673],\n",
       "        [-0.00784411,  0.03723606, -0.0391364 , ...,  0.12220044,\n",
       "          0.01768661,  0.04540838],\n",
       "        ...,\n",
       "        [ 0.09332522,  0.01864354,  0.0306711 , ..., -0.03584887,\n",
       "          0.04609067, -0.01820311],\n",
       "        [ 0.01184956, -0.09026361, -0.08486256, ..., -0.03974844,\n",
       "         -0.10788823, -0.02752244],\n",
       "        [-0.03119416, -0.11521853,  0.00831861, ..., -0.09306945,\n",
       "         -0.03388294, -0.01093579]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.13645604, -0.05812994,  0.08116466, ...,  0.0803507 ,\n",
       "          0.1517708 , -0.12684269],\n",
       "        [-0.03757641,  0.05910444,  0.10604587, ...,  0.0999862 ,\n",
       "         -0.03648807, -0.05865296],\n",
       "        [ 0.11402738,  0.08679435, -0.00819957, ...,  0.15975204,\n",
       "          0.14479509, -0.10151013],\n",
       "        ...,\n",
       "        [ 0.11130801,  0.0441677 ,  0.0530329 , ...,  0.15998384,\n",
       "          0.05862527, -0.04889344],\n",
       "        [ 0.12267497, -0.15157792, -0.14675206, ...,  0.09541163,\n",
       "         -0.03758322, -0.11027085],\n",
       "        [ 0.1448614 ,  0.11761916, -0.13892865, ..., -0.01989071,\n",
       "          0.12108672,  0.1615094 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 46us/step\n",
      "\n",
      "Evaluation loss is 14.462377227783204 and accuracy is 0.0853\n"
     ]
    }
   ],
   "source": [
    "results = m.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 8.1086 - acc: 0.4840\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 5.7738 - acc: 0.6318\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 4.5407 - acc: 0.7065\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 4.2289 - acc: 0.7282\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 4.0833 - acc: 0.7382\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 3.3220 - acc: 0.7854\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 2.1923 - acc: 0.8559\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 2.1216 - acc: 0.8606\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 2.0675 - acc: 0.8648\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 2.0189 - acc: 0.8683\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 1.9763 - acc: 0.8717\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 1.9425 - acc: 0.8743\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 1.9119 - acc: 0.8768\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 1.8866 - acc: 0.8786\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.8831 - acc: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2e321ac8>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model.fit(x=x_train, y=y_train, batch_size=128, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Trained Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n",
      "\n",
      "Evaluation loss is 0.6063993990784547 and accuracy is 0.9518\n"
     ]
    }
   ],
   "source": [
    "results = m.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordered from best to worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using the model + optimizer + weights .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.model.save('saved_mlp_model_with_w.h5') # NOTE THE model.model! IT'S IMPORTANT FOR WEIGHT ACCURACY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n",
      "\n",
      "Evaluation loss is 0.6063993990784547 and accuracy is 0.9518\n"
     ]
    }
   ],
   "source": [
    "# For some reason the model is worse than before saving. Could it be quantization? \n",
    "m2 = keras.models.load_model('saved_mlp_model_with_w.h5') \n",
    "results = m2.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.4097 - acc: 0.9679\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.3667 - acc: 0.9721\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.3282 - acc: 0.9743\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.3020 - acc: 0.9769\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2800 - acc: 0.9787\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "\n",
      "Evaluation loss is 0.47373921380256034 and accuracy is 0.9616\n"
     ]
    }
   ],
   "source": [
    "# This model can continue training!\n",
    "m2.model.fit(x=x_train, y=y_train, batch_size=128, epochs=5)\n",
    "results = m2.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using a hybrid (JSON for architecture and optimizer + .h5 for weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(keras_model):\n",
    "    if not keras_model._is_compiled:\n",
    "        raise Exception(\"Model needs to be compiled first.\")\n",
    "    return {\n",
    "        'architecture': keras_model.to_json(),\n",
    "        'optimizer': get_optimizer(keras_model)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_model = serialize_model(m.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': '{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential_24\", \"layers\": [{\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_69\", \"trainable\": true, \"batch_input_shape\": [null, 784], \"dtype\": \"float32\", \"units\": 200, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_70\", \"trainable\": true, \"units\": 200, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_71\", \"trainable\": true, \"units\": 10, \"activation\": \"softmax\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"VarianceScaling\", \"config\": {\"scale\": 1.0, \"mode\": \"fan_avg\", \"distribution\": \"uniform\", \"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.2.4\", \"backend\": \"tensorflow\"}',\n",
       " 'optimizer': {'training_config': '{\"optimizer_config\": {\"class_name\": \"SGD\", \"config\": {\"lr\": 0.0010000000474974513, \"momentum\": 0.0, \"decay\": 0.0, \"nesterov\": false}}, \"loss\": \"sparse_categorical_crossentropy\", \"metrics\": [\"accuracy\"], \"sample_weight_mode\": null, \"loss_weights\": null}'}}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_w_serializer.json', 'w') as outfile:\n",
    "    json.dump(serialized_model, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = model_from_serialized(serialized_model)\n",
    "assert m5._is_compiled, \"Model is not compiled (UNEXPECTED)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/step\n",
      "\n",
      "Evaluation loss on INITIAL MODEL is 14.725766250610352 and accuracy is 0.0699\n"
     ]
    }
   ],
   "source": [
    "results = m5.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss on INITIAL MODEL is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/step\n",
      "\n",
      "Evaluation loss on PRETRAINED MODEL is 0.6063993990784547 and accuracy is 0.9518\n"
     ]
    }
   ],
   "source": [
    "m5.load_weights('saved_mlp_weights.h5')\n",
    "results = m5.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss on PRETRAINED MODEL is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.4059 - acc: 0.9679\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.3497 - acc: 0.9724\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.3222 - acc: 0.9747\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2945 - acc: 0.9775\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2780 - acc: 0.9788\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "\n",
      "Evaluation loss is 0.4695930263816037 and accuracy is 0.9623\n"
     ]
    }
   ],
   "source": [
    "# This model can continue training like we want it.\n",
    "m5.model.fit(x=x_train, y=y_train, batch_size=128, epochs=5)\n",
    "results = m5.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using the (only) weights .h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model.save_weights('saved_mlp_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 61us/step\n",
      "\n",
      "Evaluation loss is 0.6063993990784547 and accuracy is 0.9518\n"
     ]
    }
   ],
   "source": [
    "m4 = KerasPerceptron(is_training=True)\n",
    "m4.model.load_weights('saved_mlp_weights.h5')\n",
    "results = m4.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.4066 - acc: 0.9679\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.3619 - acc: 0.9718\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.3180 - acc: 0.9748\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.3003 - acc: 0.9770\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.2822 - acc: 0.9783\n",
      "10000/10000 [==============================] - 0s 40us/step\n",
      "\n",
      "Evaluation loss is 0.46253317286027196 and accuracy is 0.9623\n"
     ]
    }
   ],
   "source": [
    "# This model can continue training, BUT only because we instantiated the KerasPerceptron class.\n",
    "m4.model.fit(x=x_train, y=y_train, batch_size=128, epochs=5)\n",
    "results = m4.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Using the .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = m.get_weights()\n",
    "np.save('./perceptron_trained.npy', trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 48us/step\n",
      "\n",
      "Evaluation loss is 0.6063993990784547 and accuracy is 0.9518\n"
     ]
    }
   ],
   "source": [
    "# The accuracy is exactly what it was before :)\n",
    "m3 = KerasPerceptron(is_training=True)\n",
    "m3.set_weights(np.load('perceptron_trained.npy'))\n",
    "results = m3.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.4039 - acc: 0.9683\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.3512 - acc: 0.9729\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.3233 - acc: 0.9758\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.3066 - acc: 0.9767\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.2885 - acc: 0.9779\n",
      "10000/10000 [==============================] - 0s 34us/step\n",
      "\n",
      "Evaluation loss is 0.4861069461776333 and accuracy is 0.9605\n"
     ]
    }
   ],
   "source": [
    "# This model can continue training, BUT only because we instantiated the KerasPerceptron class.\n",
    "m3.model.fit(x=x_train, y=y_train, batch_size=128, epochs=5)\n",
    "results = m3.model.evaluate(x=x_test, y=y_test)\n",
    "print(\"\\nEvaluation loss is {0} and accuracy is {1}\".format(*results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Keras, the best way to encode the model (and optimizer & weights) and send it around for more training is to use the `model.model.save(filepath)` function. \n",
    "\n",
    "**Please notice the** `model.model` **because it's important! Without it the weights will change and the model won't be accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact lines of code to do this would be:\n",
    "\n",
    "```python\n",
    "# Serialize the model, optimizer, and weights\n",
    "model = # The Keras model here\n",
    "filepath = 'models/filename.h5'\n",
    "model.model.save(filepath)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `serialized_model` will contain the model and optimizer while the file `weights/filename.h5` will contain the weights.\n",
    "\n",
    "To send this information through an HTTP request we would do the following:\n",
    "\n",
    "```python\n",
    "url = 'https://servers.dataagora.com/start-dml'\n",
    "metadata = {} # Some metadata\n",
    "with open(weights_filepath, 'rb') as f:\n",
    "    r = requests.post(url, files={'file': f}, data=metadata)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**: It may be that the file we're sending is too big for either the client to send or the server to receive. In these cases, we can either stream or cut the file into chunks to send. Here are some resources to accomplish this: [one](https://stackoverflow.com/a/54857411), [two](https://stackoverflow.com/a/35784072), [three](http://docs.python-requests.org/en/latest/user/quickstart/#post-a-multipart-encoded-file_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UPDATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion above is accurate and probably necessary when using a *TensorFlow.js* trainer, but it's not sufficient. *TensorFlow.js* doesn't incorporate the optimizer information when deserializing the model, so the client needs to manually compile the model. (See the `KerasAndTFJS.ipynb` for more details.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

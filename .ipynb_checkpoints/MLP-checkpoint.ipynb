{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgymh/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "X_train = np.concatenate((mnist.train.images, mnist.validation.images))\n",
    "y_train = np.concatenate((\n",
    "            np.asarray(mnist.train.labels, dtype=np.int32),\n",
    "            np.asarray(mnist.validation.labels, dtype=np.int32),\n",
    "          ))\n",
    "X_test = mnist.test.images\n",
    "y_test = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (60000, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ = X_train.reshape(-1, 784)\n",
    "y_train_ = np.eye(10)[y_train]\n",
    "X_test_ = X_test.reshape(-1, 784)\n",
    "y_test_ = np.eye(10)[y_test]\n",
    "X_train_.shape, y_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.n_input = 784\n",
    "        self.n_hidden1 = 200\n",
    "        self.n_hidden2 = 200\n",
    "        self.n_classes = 10\n",
    "\n",
    "    def preprocess_input(self, x):\n",
    "        return x\n",
    "\n",
    "    def build_model(self, input_layer):\n",
    "        #['layer1/dense/kernel:0', 'layer1/dense/bias:0', 'layer2/dense/kernel:0',\n",
    "        #'layer2/dense/bias:0', 'logits_layer/dense/kernel:0', 'logits_layer/dense/bias:0']\n",
    "        new_weights = self.new_weights if hasattr(self, 'new_weights') else None\n",
    "        layer_name = \"layer1\"\n",
    "        with tf.variable_scope(layer_name):\n",
    "            kernel_init, bias_init = None, None\n",
    "            if new_weights:\n",
    "                kernel_init = tf.constant_initializer(new_weights['/'.join([layer_name, \"dense\", \"kernel\"]) + \":0\"])\n",
    "                bias_init = tf.constant_initializer(new_weights['/'.join([layer_name, \"dense\", \"bias\"]) + \":0\"])\n",
    "            self.layer1 = tf.layers.dense(input_layer, self.n_hidden1, tf.nn.relu,\n",
    "                kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
    "        layer_name = \"layer2\"\n",
    "        with tf.variable_scope(layer_name):\n",
    "            kernel_init, bias_init = None, None\n",
    "            if new_weights:\n",
    "                kernel_init = tf.constant_initializer(new_weights['/'.join([layer_name, \"dense\", \"kernel\"]) + \":0\"])\n",
    "                bias_init = tf.constant_initializer(new_weights['/'.join([layer_name, \"dense\", \"bias\"]) + \":0\"])\n",
    "            self.layer2 = tf.layers.dense(self.layer1, self.n_hidden2, tf.nn.relu,\n",
    "                kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
    "        layer_name = \"logits_layer\"\n",
    "        with tf.variable_scope(layer_name):\n",
    "            kernel_init, bias_init = None, None\n",
    "            if new_weights:\n",
    "                kernel_init = tf.constant_initializer(new_weights['/'.join([layer_name, \"dense\", \"kernel\"]) + \":0\"])\n",
    "                bias_init = tf.constant_initializer(new_weights['/'.join([layer_name, \"dense\", \"bias\"]) + \":0\"])\n",
    "            self.logits = tf.layers.dense(self.layer2, self.n_classes,\n",
    "                kernel_initializer=kernel_init, bias_initializer=bias_init)\n",
    "\n",
    "    def build_loss(self):\n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=self.labels,\n",
    "            logits=self.logits,\n",
    "            loss_collection=tf.GraphKeys.LOSSES\n",
    "        )\n",
    "\n",
    "    def build_optimizer(self):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.optimizer = optimizer.minimize(\n",
    "            loss=self.loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            name='train_op'\n",
    "        )\n",
    "\n",
    "    def build_predictions_obj(self):\n",
    "        logits = self.logits\n",
    "        classes = tf.argmax(input=self.logits, axis=1, name=\"classes_tensor\")\n",
    "        probabilities = tf.nn.softmax(self.logits, name=\"softmax_tensor\")\n",
    "\n",
    "        tf.add_to_collection('predictions', logits)\n",
    "        tf.add_to_collection('predictions', classes)\n",
    "        tf.add_to_collection('predictions', probabilities)\n",
    "\n",
    "        self.predictions = {\n",
    "            \"logits\": logits,\n",
    "            \"classes\": classes,\n",
    "            \"probabilities\": probabilities\n",
    "        }\n",
    "\n",
    "    def build_eval_metric(self):\n",
    "        self.eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(labels=self.labels, predictions=self.predictions[\"classes\"])\n",
    "        }\n",
    "\n",
    "    def get_estimator(self, mode):\n",
    "        estimator = None\n",
    "        self.build_predictions_obj()\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            self.build_eval_metric()\n",
    "            estimator = tf.estimator.EstimatorSpec(mode=mode, predictions=self.predictions)\n",
    "        elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            self.build_loss()\n",
    "            self.build_optimizer()\n",
    "            estimator = tf.estimator.EstimatorSpec(mode=mode, loss=self.loss, train_op=self.optimizer)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            self.build_loss()\n",
    "            self.build_eval_metric()\n",
    "            estimator = tf.estimator.EstimatorSpec(mode=mode, loss=self.loss, eval_metric_ops=self.eval_metric_ops)\n",
    "        return estimator\n",
    "\n",
    "\n",
    "    def get_model(self, features, labels, mode, params):\n",
    "        \"\"\"\n",
    "        When using the Estimator API, features will come as a TF Tensor already.\n",
    "        \"\"\"\n",
    "        # Set up hyperparameters.\n",
    "        if params:\n",
    "            self.learning_rate = params.get(\"learning_rate\", None)\n",
    "            self.new_weights = params.get(\"new_weights\", None)\n",
    "\n",
    "        # Do pre-processing if necessary.\n",
    "        self.input_layer = self.preprocess_input(features[\"x\"])\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "        # def init_fn(scaffold, sess):\n",
    "        #     tf.set_random_seed(0)\n",
    "        #     sess.run(tf.global_variables_initializer())\n",
    "        # scaffold = tf.train.Scaffold(init_fn=init_fn)\n",
    "\n",
    "        # Define the model.\n",
    "        self.build_model(self.input_layer)\n",
    "        \n",
    "        #####\n",
    "#         with tf.Session().as_default() as sess:\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "#             collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#             weights = {tensor.name:sess.run(tensor) for tensor in collection}\n",
    "#         print(weights['layer1/dense/bias:0'][:5])\n",
    "        #####\n",
    "        \n",
    "        # Build and return the estimator.\n",
    "        return self.get_estimator(mode)\n",
    "\n",
    "    def load_weights(self, new_weights, metagraph_file, checkpoint_dir):\n",
    "        tf.reset_default_graph()\n",
    "        with tf.Session().as_default() as sess:\n",
    "            new_saver = tf.train.import_meta_graph(metagraph_file)\n",
    "            # To load non-trainable variables and prevent errors...\n",
    "            # we restore them if they are found, or initialize them otherwise.\n",
    "            try:\n",
    "                new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            except:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            for tensor in collection:\n",
    "                assign_op = tensor.assign(new_weights[tensor.name])\n",
    "                sess.run(assign_op)\n",
    "\n",
    "            save_path = new_saver.save(sess, checkpoint_dir + \"model.ckpt\")\n",
    "\n",
    "    def get_weights(self, latest_checkpoint):\n",
    "        tf.reset_default_graph()\n",
    "        graph = tf.Graph()\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "            new_saver = tf.train.import_meta_graph(latest_checkpoint + '.meta')\n",
    "            new_saver.restore(sess, latest_checkpoint)\n",
    "            collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            weights = {tensor.name:sess.run(tensor) for tensor in collection}\n",
    "        return weights\n",
    "\n",
    "    def sum_weights(self, weights1, weights2):\n",
    "        new_weights = {}\n",
    "        for key1, key2 in zip(sorted(weights1.keys()), sorted(weights2.keys())):\n",
    "            assert key1 == key2, 'Error with keys'\n",
    "            new_weights[key1] = weights1[key1] + weights2[key2]\n",
    "        return new_weights\n",
    "\n",
    "    def scale_weights(self, weights, factor):\n",
    "        new_weights = {}\n",
    "        for key, value in weights.items():\n",
    "            new_weights[key] = value * factor\n",
    "        return new_weights\n",
    "\n",
    "    def inverse_scale_weights(self, weights, factor):\n",
    "        new_weights = {}\n",
    "        for key, value in weights.items():\n",
    "            new_weights[key] = value / factor\n",
    "        return new_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the Estimator\n",
    "# model = Perceptron()\n",
    "# classifier = tf.estimator.Estimator(model_fn=model.get_model, model_dir=\"./mnist/\")\n",
    "\n",
    "# # Set up logging for predictions\n",
    "# tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "# logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-73-6b57660b5e1e>, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-6b57660b5e1e>\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ESTIMATOR WAY\n",
    "1- Server sends its 'input_fn()' defining the model and initial weights to clients \n",
    "    [yes]\n",
    "2- Clients create an estimator with the weights, train on their data, and send back the new weights to the server \n",
    "    [yes -- put weights in folder, create estimator with checkpoint, run fit on estimator, sends the latest checkpoint]\n",
    "3- As the server receives the new weights, it averages the weights; once done, it evaluates the model on a test set \n",
    "    [yes -- as weights are received we average them; once done, we load the model, assign the weights, save the model,\n",
    "    under new name, and run an eval on the test set]\n",
    "\n",
    "\n",
    "TRADITIONAL WAY\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# CLIENT\n",
    "def train_model(config):\n",
    "    \"\"\"\n",
    "    Loads the model and weights found in server-checkpoints/ (if found), trains using the hyperparameters \n",
    "    in config, then returns a list of new checkpoints after training. \n",
    "    \"\"\"\n",
    "    def make_weights_dict(sess, collection):\n",
    "        return {tensor.name:sess.run(tensor) for tensor in collection}\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        #new_saver = tf.train.import_meta_graph(metagraph)\n",
    "        new_saver = tf.train.import_meta_graph(tf.train.latest_checkpoint('./mnist/') + '.meta')\n",
    "        try:\n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint('./mnist/'))\n",
    "        except e:\n",
    "            # Couldn't load model.\n",
    "            print(\"Error...\")\n",
    "            raise e \n",
    "        \n",
    "        input = graph.get_collection('input_tensor:0')\n",
    "        print(input)\n",
    "        \n",
    "        logits, classes, probabilities = graph.get_collection('predictions')\n",
    "        print(logits)\n",
    "        \n",
    "        collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        print(make_weights_dict(sess, collection))\n",
    "        \n",
    "        for i in range(20000):\n",
    "            batch = mnist.train.next_batch(50)\n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            if i % 100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "                print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            if i % 1000 == 0:\n",
    "                # test accuracy\n",
    "            \n",
    "\n",
    "        print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "# SERVER\n",
    "def aggregate_models(config):\n",
    "    \"\"\"\n",
    "    Loads the models found in updates/\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "#train_model(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "\n",
    "# print all tensors in checkpoint file\n",
    "#chkp.print_tensors_in_checkpoint_file(tf.train.latest_checkpoint('./mnist/'), tensor_name='', all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07752929 0.0894733  0.05860019 0.10548263 0.00462598]\n"
     ]
    }
   ],
   "source": [
    "def get_initial_weights():\n",
    "    tf.reset_default_graph()\n",
    "    m = Perceptron()\n",
    "    initial = tf.placeholder(tf.float32, shape=(None, 28*28), name=\"input_tensor\")\n",
    "    _ = m.get_model(features={\"x\": initial}, labels=None, mode='predict', params=None)\n",
    "    with tf.Session().as_default() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        weights = {tensor.name:sess.run(tensor) for tensor in collection}\n",
    "    tf.reset_default_graph()\n",
    "    return weights\n",
    "\n",
    "initial_weights = get_initial_weights()\n",
    "print(initial_weights['layer1/dense/bias:0'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the Estimator\n",
    "params = {'learning_rate': 0.001, 'new_weights': initial_weights}\n",
    "model = Perceptron()\n",
    "classifier1 = tf.estimator.Estimator(\n",
    "    model_fn=model.get_model, \n",
    "    model_dir=\"./mnist1/lol/\",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train_},\n",
    "    y=y_train,\n",
    "    batch_size=1,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "classifier1.train(input_fn=train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './mnist2/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11cf44e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./mnist2/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.6577064, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ./mnist2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.021074183.\n",
      "CPU times: user 1.58 s, sys: 264 ms, total: 1.85 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the Estimator\n",
    "params = {'learning_rate': 0.01, 'new_weights': initial_weights}\n",
    "model = Perceptron()\n",
    "classifier2 = tf.estimator.Estimator(\n",
    "    model_fn=model.get_model, \n",
    "    model_dir=\"./mnist2/\",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train_},\n",
    "    y=y_train,\n",
    "    batch_size=1,\n",
    "    num_epochs=None,\n",
    "    shuffle=False\n",
    ")\n",
    "classifier2.train(input_fn=train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-01-00:18:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./mnist1/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-01-00:18:37\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.5769, global_step = 100, loss = 1.3338704\n",
      "{'accuracy': 0.5769, 'loss': 1.3338704, 'global_step': 100}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test_},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "eval_results = classifier1.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-01-00:18:38\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./mnist2/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-01-00:18:39\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.3963, global_step = 100, loss = 1.7706457\n",
      "{'accuracy': 0.3963, 'loss': 1.7706457, 'global_step': 100}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test_},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "eval_results = classifier2.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist1/model.ckpt-100\n",
      "INFO:tensorflow:Restoring parameters from ./mnist2/model.ckpt-100\n",
      "199210 [-0.00913986  0.10991648  0.04031864  0.09077774 -0.10459123]\n",
      "199210 [-0.06314074  0.06910972 -0.00736872  0.05201993 -0.15259027]\n",
      "['layer1/dense/kernel:0', 'layer1/dense/bias:0', 'layer2/dense/kernel:0', 'layer2/dense/bias:0', 'logits_layer/dense/kernel:0', 'logits_layer/dense/bias:0']\n"
     ]
    }
   ],
   "source": [
    "def import_weights_as_np(path):\n",
    "    tf.reset_default_graph()\n",
    "    weights = []\n",
    "    with tf.Session().as_default() as sess:\n",
    "        new_saver = tf.train.import_meta_graph(tf.train.latest_checkpoint(path) + '.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint(path))\n",
    "        collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        weights = {tensor.name:sess.run(tensor) for tensor in collection}\n",
    "    return weights\n",
    "        \n",
    "weights1 = import_weights_as_np('./mnist1/')\n",
    "weights2 = import_weights_as_np('./mnist2/')\n",
    "\n",
    "print(sum(w.size for _, w in weights1.items()), weights1['layer1/dense/bias:0'][:5])\n",
    "print(sum(w.size for _, w in weights2.items()), weights2['layer1/dense/bias:0'][:5])\n",
    "print([k for k, _ in weights2.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199210 [-0.0361403   0.0895131   0.01647496  0.07139883 -0.12859075]\n"
     ]
    }
   ],
   "source": [
    "def sum_weights(weights1, weights2):\n",
    "    new_weights = {}\n",
    "    for key1, key2 in zip(sorted(weights1.keys()), sorted(weights2.keys())):\n",
    "        assert key1 == key2, 'Error with keys'\n",
    "        new_weights[key1] = (weights1[key1] + weights2[key2])/2\n",
    "    return new_weights\n",
    "\n",
    "new_weights = sum_weights(weights1, weights2)\n",
    "print(sum(w.size for _, w in new_weights.items()), new_weights['layer1/dense/bias:0'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './mnist3/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11d679630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./mnist3/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.757868, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ./mnist3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0002796259.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-01-00:21:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./mnist3/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-01-00:21:14\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.5448, global_step = 100, loss = 1.3598974\n",
      "{'accuracy': 0.5448, 'loss': 1.3598974, 'global_step': 100}\n",
      "CPU times: user 1.95 s, sys: 159 ms, total: 2.11 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the Estimator\n",
    "model = Perceptron()\n",
    "params = {'learning_rate': 0.01, 'new_weights': new_weights}\n",
    "classifier3 = tf.estimator.Estimator(\n",
    "    model_fn=model.get_model, \n",
    "    model_dir=\"./mnist3/\",\n",
    "    params=params\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train_},\n",
    "    y=y_train,\n",
    "    batch_size=1,\n",
    "    num_epochs=None,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "classifier3.train(input_fn=train_input_fn, steps=100)\n",
    "\n",
    "\n",
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_test_},\n",
    "    y=y_test,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "eval_results = classifier3.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist/model.ckpt\n",
      "Model saved in path: ./mnist/model.ckpt-1337\n"
     ]
    }
   ],
   "source": [
    "def load_weights(weights, round_num):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session().as_default() as sess:\n",
    "        new_saver = tf.train.import_meta_graph(tf.train.latest_checkpoint('./mnist/') + '.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./mnist/')) # to load non-trainable variables \n",
    "        \n",
    "        collection = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        for tensor in collection:\n",
    "            assign_op = tensor.assign(weights[tensor.name])\n",
    "            sess.run(assign_op)\n",
    "        \n",
    "        save_path = new_saver.save(sess, \"./mnist/model.ckpt\", global_step=round_num)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "load_weights(new_weights, 1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"node\": [\\n    {\\n      \"name\": \"Placeholder\",\\n '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.protobuf import json_format\n",
    "\n",
    "def export_metagraph_as_json(model, hparams):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        m = model(hparams)\n",
    "        m.build_model(tf.placeholder(tf.float32, (None, 28*28)))\n",
    "        m.build_predictions_obj()\n",
    "        graph_def = graph.as_graph_def()\n",
    "        json_string = json_format.MessageToJson(graph_def)\n",
    "    return json_string\n",
    "\n",
    "export_metagraph_as_json(Perceptron, hparams)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mnist/model.ckpt-23613'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def export_metagraph():\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session().as_default() as sess:\n",
    "        new_saver = tf.train.import_meta_graph(tf.train.latest_checkpoint('./mnist/') + '.meta')\n",
    "#         m = model(hparams)\n",
    "#         m.build_model(tf.placeholder(tf.float32, (None, 28*28), name='input_tensor'))\n",
    "#         m.build_predictions_obj()\n",
    "        meta_graph_def = tf.train.export_meta_graph()\n",
    "    return meta_graph_def\n",
    "\n",
    "#mgraph = export_metagraph()\n",
    "#metagraph = export_metagraph(Perceptron, hparams)\n",
    "tf.train.latest_checkpoint('./mnist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist/model.ckpt-23613\n",
      "[<tf.Variable 'global_step:0' shape=() dtype=int64_ref>, <tf.Variable 'layer1/dense/kernel:0' shape=(784, 200) dtype=float32_ref>, <tf.Variable 'layer1/dense/bias:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'layer2/dense/kernel:0' shape=(200, 200) dtype=float32_ref>, <tf.Variable 'layer2/dense/bias:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'logits_layer/dense/kernel:0' shape=(200, 10) dtype=float32_ref>, <tf.Variable 'logits_layer/dense/bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'layer1/dense/kernel/Adam:0' shape=(784, 200) dtype=float32_ref>, <tf.Variable 'layer1/dense/kernel/Adam_1:0' shape=(784, 200) dtype=float32_ref>, <tf.Variable 'layer1/dense/bias/Adam:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'layer1/dense/bias/Adam_1:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'layer2/dense/kernel/Adam:0' shape=(200, 200) dtype=float32_ref>, <tf.Variable 'layer2/dense/kernel/Adam_1:0' shape=(200, 200) dtype=float32_ref>, <tf.Variable 'layer2/dense/bias/Adam:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'layer2/dense/bias/Adam_1:0' shape=(200,) dtype=float32_ref>, <tf.Variable 'logits_layer/dense/kernel/Adam:0' shape=(200, 10) dtype=float32_ref>, <tf.Variable 'logits_layer/dense/kernel/Adam_1:0' shape=(200, 10) dtype=float32_ref>, <tf.Variable 'logits_layer/dense/bias/Adam:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'logits_layer/dense/bias/Adam_1:0' shape=(10,) dtype=float32_ref>]\n",
      "Tensor(\"enqueue_input/Placeholder_1:0\", dtype=float32, device=/device:CPU:0)\n",
      "Tensor(\"logits_layer/dense/BiasAdd:0\", shape=(100, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def build_latest_graph():\n",
    "    tf.reset_default_graph()\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        #new_saver = tf.train.import_meta_graph(metagraph)\n",
    "        new_saver = tf.train.import_meta_graph(tf.train.latest_checkpoint('./mnist/') + '.meta')\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint('./mnist/'))\n",
    "        \n",
    "        print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "        \n",
    "        input = loaded_graph.get_tensor_by_name('enqueue_input/Placeholder_1:0')\n",
    "        #input = loaded_graph.get_collection('enqueue_input')\n",
    "        #input = loaded_graph.get_collection('input_tensor:0')\n",
    "        print(input)\n",
    "        \n",
    "        logits, classes, probabilities = loaded_graph.get_collection('predictions')\n",
    "        print(logits)\n",
    "        \n",
    "#         def get_model(features, labels, mode):\n",
    "#             return tf.estimator.EstimatorSpec(mode=mode, predictions=classes)\n",
    "            \n",
    "        \n",
    "#         hparams = {'learning_rate': 0.001}\n",
    "#         model = Perceptron(hparams)\n",
    "#         classifier = tf.estimator.Estimator(model_fn=get_model, model_dir=\"mnist2/\")\n",
    "#         # Evaluate the model and print results\n",
    "#         eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#             x={\"x\": X_test_},\n",
    "#             y=y_test,\n",
    "#             num_epochs=1,\n",
    "#             shuffle=False\n",
    "#         )\n",
    "#         eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "#         print(eval_results)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         pred = sess.run(classes, feed_dict={input: X_test_[:10]})\n",
    "#         print(pred, y_test[:10])\n",
    "        \n",
    "        #print(sess.run(predictions[0]))\n",
    "    #print(sess.run(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)))\n",
    "    #   for step in xrange(1000000):\n",
    "    #     sess.run(train_op)\n",
    "\n",
    "build_latest_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "m = Perceptron(hparams)\n",
    "m.build_model(tf.placeholder(tf.float32, (None, 28*28)))\n",
    "\n",
    "weights = []\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  #new_saver = tf.train.import_meta_graph('mnist/model.ckpt-23605.meta')\n",
    "  #new_saver.restore(sess, tf.train.latest_checkpoint('mnist'))\n",
    "  #saver.restore(sess, \"./mnist/model.ckpt\") \n",
    "    \n",
    "  ckpt = tf.train.get_checkpoint_state(\"./mnist/model.ckpt-23604\")\n",
    "  print(ckpt)\n",
    "  \n",
    "    \n",
    "  print(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "  weights = sess.run(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_model() missing 1 required positional argument: 'input_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-d61c059f5c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-d61c059f5c71>\u001b[0m in \u001b[0;36mprepare_for_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./mnist/lol.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-9ee8d2d9546e>\u001b[0m in \u001b[0;36mbuild_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_model() missing 1 required positional argument: 'input_layer'"
     ]
    }
   ],
   "source": [
    "def prepare_for_training():\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session().as_default() as sess:\n",
    "        m = Perceptron(hparams)\n",
    "        m.build_all()\n",
    "        tf.train.import_meta_graph(filename='./mnist/lol.meta')\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "prepare_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mnist/model.ckpt-2'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('./mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
